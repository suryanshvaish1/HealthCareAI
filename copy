import os
from dotenv import load_dotenv
import gradio as gr

# Load environment variables from .env
load_dotenv()

# Import your custom modules
from brain_of_the_doctor import encode_image, analyze_image_with_query
from voice_of_the_patient import transcribe_with_groq
from voice_of_the_doctor import text_to_speech_with_elevenlabs

# System prompt for the AI Doctor
SYSTEM_PROMPT = """You have to act as a professional doctor, i know you are not but this is for learning purpose. 
            What's in this image?. Do you find anything wrong with it medically? 
            If you make a differential, suggest some remedies for them. Donot add any numbers or special characters in 
            your response. Your response should be in one long paragraph. Also always answer as if you are answering to a real person.
            Donot say 'In the image I see' but say 'With what I see, I think you have ....'
            Dont respond as an AI model in markdown, your answer should mimic that of an actual doctor not an AI bot, 
            Keep your answer concise (max 2 sentences). No preamble, start your answer right away please"""

def process_inputs(audio_filepath, image_filepath):
    # 1. Validation
    if audio_filepath is None:
        return "No audio provided", "Please describe your symptoms via audio.", None

    # 2. Speech-to-Text (Patient Voice)
    try:
        speech_to_text_output = transcribe_with_groq(
            GROQ_API_KEY=os.environ.get("GROQ_API_KEY"), 
            audio_filepath=audio_filepath,
            stt_model="whisper-large-v3"
        )
    except Exception as e:
        return f"STT Error: {str(e)}", "Speech transcription failed.", None

    # 3. Vision Analysis (Doctor's Brain) using the verified active model
    doctor_response = ""
    if image_filepath:
        try:
            # Using your active Llama 4 Scout model for fast, accurate vision analysis
            doctor_response = analyze_image_with_query(
                query=SYSTEM_PROMPT + speech_to_text_output,
                encoded_image=encode_image(image_filepath),
                model="meta-llama/llama-4-scout-17b-16e-instruct"
            )
        except Exception as e:
            print(f"Vision Error: {e}")
            doctor_response = "I encountered an error connecting to my vision systems. Please check your API key."
    else:
        doctor_response = "No image provided for me to analyze."

    # 4. Text-to-Speech (Doctor's Voice)
    try:
        voice_of_doctor_path = text_to_speech_with_elevenlabs(
            input_text=doctor_response, 
            output_filepath="final.mp3"
        ) 
    except Exception as e:
        # Prevents the UI from hanging if there is a permission error
        print(f"ElevenLabs Error: {e}")
        voice_of_doctor_path = None

    return speech_to_text_output, doctor_response, voice_of_doctor_path

# --- UI Layout matches your previous successful run ---
with gr.Blocks(title="AI Doctor 2.0") as demo:
    gr.Markdown("# üè• AI Doctor: Vision & Voice Analysis")
    gr.Markdown("Upload a medical image and explain your symptoms using the microphone.")
    
    with gr.Row():
        with gr.Column():
            audio_input = gr.Audio(sources=["microphone"], type="filepath", label="Describe Symptoms")
            image_input = gr.Image(type="filepath", label="Medical Image")
            submit_btn = gr.Button("Submit", variant="primary")
            
        with gr.Column():
            stt_output = gr.Textbox(label="Speech to Text")
            response_output = gr.Textbox(label="Doctor's Response")
            voice_output = gr.Audio(label="Doctor's Voice")

    submit_btn.click(
        fn=process_inputs,
        inputs=[audio_input, image_input],
        outputs=[stt_output, response_output, voice_output]
    )

if __name__ == "__main__":
    demo.launch(debug=True)







    # if you dont use pipenv uncomment the following:
from dotenv import load_dotenv
load_dotenv()

#Step1: Setup GROQ API key
import os

GROQ_API_KEY=os.environ.get("GROQ_API_KEY")

#Step2: Convert image to required format
import base64


#image_path="acne.jpg"

def encode_image(image_path):   
    image_file=open(image_path, "rb")
    return base64.b64encode(image_file.read()).decode('utf-8')

#Step3: Setup Multimodal LLM 
from groq import Groq

query="Is there something wrong with my face?"
#model = "meta-llama/llama-4-maverick-17b-128e-instruct"
model="meta-llama/llama-4-scout-17b-16e-instruct"
#model = "meta-llama/llama-4-scout-17b-16e-instruct"
#model="llama-3.2-90b-vision-preview" #Deprecated

def analyze_image_with_query(query, model, encoded_image):
    client=Groq()  
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text", 
                    "text": query
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{encoded_image}",
                    },
                },
            ],
        }]
    chat_completion=client.chat.completions.create(
        messages=messages,
        model=model
    )

    return chat_completion.choices[0].message.content
